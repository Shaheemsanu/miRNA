{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cbGVREEatQIa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from keras import utils\n",
    "import numpy.matlib\n",
    "\n",
    "SD = np.loadtxt(\"SD.txt\", dtype=float, delimiter=\" \")\n",
    "SM = np.loadtxt(\"SM.txt\", dtype=float, delimiter=\" \")\n",
    "A = np.loadtxt(\"interaction.txt\", dtype=int, delimiter=\" \")\n",
    "interacation = np.transpose(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shahe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\shahe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "R_B = np.matlib.repmat(interacation, 495, 1)\n",
    "sm = np.repeat(SM, repeats=383, axis=0)\n",
    "train_m = np.concatenate((sm, R_B), axis=1)  # (189585,990)\n",
    "\n",
    "R_A = np.repeat(A, repeats=383, axis=0)\n",
    "sd = np.matlib.repmat(SD, 495, 1)\n",
    "train_s = np.concatenate((R_A, sd), axis=1)  # (189585,766)\n",
    "label = A.reshape((189585, 1))\n",
    "\n",
    "# Initialize LabelEncoder and fit it to the labels\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(label)\n",
    "\n",
    "# Transform the labels to integer values\n",
    "ys = encoder.transform(label).astype(np.int32)\n",
    "\n",
    "# Convert to categorical (one-hot encoding) if needed\n",
    "ys = to_categorical(ys)\n",
    "\n",
    "nm = np.arange(len(ys))\n",
    "ys = ys[nm]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.       0.249269 0.093465 ... 0.       0.       0.      ]\n",
      " [1.       0.249269 0.093465 ... 0.       0.       0.      ]\n",
      " [1.       0.249269 0.093465 ... 0.       0.       0.      ]\n",
      " ...\n",
      " [0.077888 0.064907 0.161507 ... 0.       0.       0.      ]\n",
      " [0.077888 0.064907 0.161507 ... 0.       0.       0.      ]\n",
      " [0.077888 0.064907 0.161507 ... 0.       0.       0.      ]]\n",
      "[[1.       0.       0.       ... 0.86843  0.49394  0.568774]\n",
      " [1.       0.       0.       ... 0.754171 0.264119 0.568774]\n",
      " [1.       0.       0.       ... 0.530038 0.203341 0.61034 ]\n",
      " ...\n",
      " [0.       0.       0.       ... 1.       0.568774 0.654945]\n",
      " [0.       0.       0.       ... 0.568774 1.       0.49394 ]\n",
      " [0.       0.       0.       ... 0.654945 0.49394  1.      ]]\n"
     ]
    }
   ],
   "source": [
    "print(train_m) \n",
    "print(train_s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9x8JkcxauHN8",
    "outputId": "69972193-ae68-4114-ee08-dd877e5bae60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 13ms/step - loss: 0.0159\n",
      "Epoch 2/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 12ms/step - loss: 0.0036\n",
      "Epoch 3/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.0029\n",
      "Epoch 4/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 0.0025\n",
      "Epoch 5/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 12ms/step - loss: 0.0027\n",
      "Epoch 6/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.0025\n",
      "Epoch 7/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - loss: 0.0020\n",
      "Epoch 8/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - loss: 0.0019\n",
      "Epoch 9/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - loss: 0.0019\n",
      "Epoch 10/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - loss: 0.0018\n",
      "Epoch 11/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - loss: 0.0018\n",
      "Epoch 12/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 14ms/step - loss: 0.0018\n",
      "Epoch 13/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.0019\n",
      "Epoch 14/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 11ms/step - loss: 0.0019\n",
      "Epoch 15/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 14ms/step - loss: 0.0018\n",
      "\u001b[1m5925/5925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "encoding_dim = 64\n",
    "input_mdata = layers.Input(shape=(990,))\n",
    "\n",
    "encoded = layers.Dense(350, activation='relu')(input_mdata)\n",
    "encoded = layers.Dense(250, activation='relu')(encoded)\n",
    "encoded = layers.Dense(100, activation='relu')(encoded)\n",
    "miRNA_encoder_output = layers.Dense(encoding_dim)(encoded)\n",
    "\n",
    "decoded = layers.Dense(100, activation='relu')(miRNA_encoder_output)\n",
    "decoded = layers.Dense(250, activation='relu')(decoded)\n",
    "decoded = layers.Dense(350, activation='relu')(decoded)\n",
    "decoded = layers.Dense(990, activation='sigmoid')(decoded)  # or 'tanh' if data is scaled accordingly\n",
    "\n",
    "autoencoder = models.Model(input_mdata, decoded)\n",
    "encoder = models.Model(input_mdata, miRNA_encoder_output)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(train_m, train_m, epochs=15, batch_size=100, shuffle=True)\n",
    "miRNA_encoded_datas = encoder.predict(train_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlmGHegww6O6",
    "outputId": "0c5cfe41-aa71-403c-f4d2-ada761a8a3b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 13ms/step - loss: 0.0185\n",
      "Epoch 2/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.0048\n",
      "Epoch 3/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 11ms/step - loss: 0.0037\n",
      "Epoch 4/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 16ms/step - loss: 0.0034\n",
      "Epoch 5/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - loss: 0.0033\n",
      "Epoch 6/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - loss: 0.0028\n",
      "Epoch 7/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.0026\n",
      "Epoch 8/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.0026\n",
      "Epoch 9/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 0.0024\n",
      "Epoch 10/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - loss: 0.0024\n",
      "Epoch 11/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 0.0023\n",
      "Epoch 12/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 14ms/step - loss: 0.0023\n",
      "Epoch 13/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 15ms/step - loss: 0.0023\n",
      "Epoch 14/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 16ms/step - loss: 0.0022\n",
      "Epoch 15/15\n",
      "\u001b[1m1896/1896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0022\n",
      "\u001b[1m5925/5925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "encoding_dim = 64\n",
    "input_ddata = layers.Input(shape=(766,))\n",
    "\n",
    "encoded = layers.Dense(350, activation='relu')(input_ddata)\n",
    "encoded = layers.Dense(250, activation='relu')(encoded)\n",
    "encoded = layers.Dense(100, activation='relu')(encoded)\n",
    "disease_encoder_output = layers.Dense(encoding_dim)(encoded)\n",
    "\n",
    "decoded = layers.Dense(100, activation='relu')(disease_encoder_output)\n",
    "decoded = layers.Dense(250, activation='relu')(decoded)\n",
    "decoded = layers.Dense(350, activation='relu')(decoded)\n",
    "decoded = layers.Dense(766, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = models.Model(input_ddata, decoded)\n",
    "encoder = models.Model(input_ddata, disease_encoder_output)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(train_s, train_s, epochs=15, batch_size=100, shuffle=True)\n",
    "disease_encoded_datas = encoder.predict(train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save(\"autoencoder_disease_model.h5\")  # Save the entire autoencoder model\n",
    "encoder.save(\"encoder_disease_model.h5\")          # Save only the encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoencoder model\n",
    "loaded_autoencoder = tf.keras.models.load_model(\"autoencoder_disease_model.h5\")\n",
    "\n",
    "# Load the encoder model\n",
    "loaded_encoder = tf.keras.models.load_model(\"encoder_disease_model.h5\")\n",
    "\n",
    "# Use the loaded encoder for predictions\n",
    "miRNA_encoded_datas = loaded_encoder.predict(train_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'miRNA_encoded_datas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score, confusion_matrix, matthews_corrcoef\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((\u001b[43mmiRNA_encoded_datas\u001b[49m, disease_encoded_datas))\n\u001b[0;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize lists to store evaluation metrics\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'miRNA_encoded_datas' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score, confusion_matrix, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.hstack((miRNA_encoded_datas, disease_encoded_datas))\n",
    "y = A.flatten()\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "accuracy_results, sensitivity_results, specificity_results, precision_results, mcc_results = [], [], [], [], []\n",
    "y_true, y_scores = [], []\n",
    "confusion_matrices = []\n",
    "\n",
    "# Define KFold cross-validation (5-fold or 10-fold)\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)  # Use n_splits=10 for 10-fold CV\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, val_index in kf.split(X):\n",
    "    x_train, x_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Initialize RandomForestClassifier with class weights to handle imbalance\n",
    "    clf = RandomForestClassifier(n_estimators=10, max_depth=20,min_samples_split=5,min_samples_leaf=3,random_state=42,class_weight=\"balanced\")\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Get predictions and probabilities for evaluation\n",
    "    y_pred = clf.predict(x_val)\n",
    "    y_pred_proba = clf.predict_proba(x_val)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "    # Store true labels and predicted probabilities for ROC-AUC calculation\n",
    "    y_true.extend(y_val)\n",
    "    y_scores.extend(y_pred_proba)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)  # Sensitivity\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    \n",
    "    # Calculate specificity using the confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    # Append each fold's metrics to results lists\n",
    "    accuracy_results.append(accuracy)\n",
    "    sensitivity_results.append(recall)  # Sensitivity\n",
    "    specificity_results.append(specificity)\n",
    "    precision_results.append(precision)\n",
    "    mcc_results.append(mcc)\n",
    "    \n",
    "    # Calculate confusion matrix for the fold and store it\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    confusion_matrices.append(cm)\n",
    "\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "avg_accuracy = sum(accuracy_results) / len(accuracy_results)\n",
    "avg_sensitivity = sum(sensitivity_results) / len(sensitivity_results)\n",
    "avg_specificity = sum(specificity_results) / len(specificity_results)\n",
    "avg_precision = sum(precision_results) / len(precision_results)\n",
    "avg_mcc = sum(mcc_results) / len(mcc_results)\n",
    "roc_auc = roc_auc_score(y_true, y_scores)  # Overall ROC-AUC score\n",
    "\n",
    "final_cm = confusion_matrices[-1]\n",
    "print(f\"Confusion Matrix:\\n{final_cm}\\n\")\n",
    "\n",
    "# Display results\n",
    "print(f\"Average Accuracy (Acc): {avg_accuracy:.4f}\")\n",
    "print(f\"Average Sensitivity (Sen): {avg_sensitivity:.4f}\")\n",
    "print(f\"Average Specificity (Spe): {avg_specificity:.4f}\")\n",
    "print(f\"Average Precision (Pre): {avg_precision:.4f}\")\n",
    "print(f\"Average Matthews Correlation Coefficient (Mcc): {avg_mcc:.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('roc_curve.png') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# X = np.hstack((miRNA_encoded_datas, disease_encoded_datas))\n",
    "# y = ys.flatten()\n",
    "\n",
    "\n",
    "\n",
    "# num_folds = 5\n",
    "# kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# accuracy_results = []\n",
    "# precision_results = []\n",
    "# recall_results = []\n",
    "# f1_results = []\n",
    "# y_true = []\n",
    "# y_scores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #avg_sensitivity mcc \n",
    "\n",
    "\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, matthews_corrcoef\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# # Initialize lists to store evaluation metrics\n",
    "# accuracy_results, sensitivity_results, specificity_results, precision_results, mcc_results = [], [], [], [], []\n",
    "# y_true, y_scores = [], []\n",
    "# confusion_matrices = []\n",
    "# threshold = 0.4\n",
    "\n",
    "# # Define KFold cross-validation (5-fold or 10-fold)\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)  # Use n_splits=10 for 10-fold CV\n",
    "\n",
    "# # Cross-validation loop\n",
    "# for train_index, val_index in kf.split(X):\n",
    "#     x_train, x_val = X[train_index], X[val_index]\n",
    "#     y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "#     # Initialize RandomForestClassifier with class weights to handle imbalance\n",
    "#     clf = RandomForestClassifier(n_estimators=300, max_depth=25,min_samples_split=20,min_samples_leaf=10,class_weight=\"balanced\")\n",
    "#     clf.fit(x_train, y_train)\n",
    "\n",
    "#     # Get predictions and probabilities for evaluation\n",
    "#     y_pred = clf.predict(x_val)\n",
    "#     y_pred_proba = clf.predict_proba(x_val)[:, 1]  # Probabilities for the positive class\n",
    "#     y_pred_custom = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "#     # Store true labels and predicted probabilities for ROC-AUC calculation\n",
    "#     y_true.extend(y_val)\n",
    "#     y_scores.extend(y_pred_proba)\n",
    "\n",
    "#     # Calculate evaluation metrics\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "#     precision = precision_score(y_val, y_pred)\n",
    "#     recall = recall_score(y_val, y_pred)  # Sensitivity\n",
    "#     mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    \n",
    "#     # Calculate specificity using the confusion matrix\n",
    "#     tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "#     specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "#     # Append each fold's metrics to results lists\n",
    "#     accuracy_results.append(accuracy)\n",
    "#     sensitivity_results.append(recall)  # Sensitivity\n",
    "#     specificity_results.append(specificity)\n",
    "#     precision_results.append(precision)\n",
    "#     mcc_results.append(mcc)\n",
    "    \n",
    "#     # Calculate confusion matrix for the fold and store it\n",
    "#     cm = confusion_matrix(y_val, y_pred)\n",
    "#     confusion_matrices.append(cm)\n",
    "\n",
    "# # Calculate average metrics across all folds\n",
    "# avg_accuracy = sum(accuracy_results) / len(accuracy_results)\n",
    "# avg_sensitivity = sum(sensitivity_results) / len(sensitivity_results)\n",
    "# avg_specificity = sum(specificity_results) / len(specificity_results)\n",
    "# avg_precision = sum(precision_results) / len(precision_results)\n",
    "# avg_mcc = sum(mcc_results) / len(mcc_results)\n",
    "# roc_auc = roc_auc_score(y_true, y_scores)  # Overall ROC-AUC score\n",
    "\n",
    "# # Display results\n",
    "# print(f\"Average Accuracy (Acc): {avg_accuracy:.4f}\")\n",
    "# print(f\"Average Sensitivity (Sen): {avg_sensitivity:.4f}\")\n",
    "# print(f\"Average Specificity (Spe): {avg_specificity:.4f}\")\n",
    "# print(f\"Average Precision (Pre): {avg_precision:.4f}\")\n",
    "# print(f\"Average Matthews Correlation Coefficient (Mcc): {avg_mcc:.4f}\")\n",
    "# print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# # Print confusion matrices for each fold\n",
    "# for i, cm in enumerate(confusion_matrices):\n",
    "#     print(f\"Confusion Matrix for Fold {i + 1}:\\n{cm}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # Plot ROC curve\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, color='blue', label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.0])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.savefig('roc_curve.png')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balanced in randomforestclassifier\n",
    "\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # Initialize lists to store evaluation metrics\n",
    "# accuracy_results, precision_results, recall_results, f1_results = [], [], [], []\n",
    "# y_true, y_scores = [], []\n",
    "\n",
    "# # KFold cross-validation\n",
    "# for train_index, val_index in kf.split(X):\n",
    "#     x_train, x_val = X[train_index], X[val_index]\n",
    "#     y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "#     # Add class weights to the classifier\n",
    "#     clf = RandomForestClassifier(n_estimators=300, class_weight=\"balanced\")\n",
    "#     clf.fit(x_train, y_train)\n",
    "\n",
    "#     # Get predictions and probabilities\n",
    "#     y_pred = clf.predict(x_val)\n",
    "#     y_pred_proba = clf.predict_proba(x_val)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "#     # Store true labels and predicted probabilities for ROC calculation\n",
    "#     y_true.extend(y_val)\n",
    "#     y_scores.extend(y_pred_proba)\n",
    "\n",
    "#     # Calculate metrics at default threshold (0.5)\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "#     precision = precision_score(y_val, y_pred)\n",
    "#     recall = recall_score(y_val, y_pred)\n",
    "#     f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "#     # Append metrics to the results lists\n",
    "#     accuracy_results.append(accuracy)\n",
    "#     precision_results.append(precision)\n",
    "#     recall_results.append(recall)\n",
    "#     f1_results.append(f1)\n",
    "\n",
    "# # Calculate average metrics across all folds\n",
    "# avg_accuracy = sum(accuracy_results) / len(accuracy_results)\n",
    "# avg_precision = sum(precision_results) / len(precision_results)\n",
    "# avg_recall = sum(recall_results) / len(recall_results)\n",
    "# avg_f1 = sum(f1_results) / len(f1_results)\n",
    "# roc_auc = roc_auc_score(y_true, y_scores)  # Overall ROC-AUC score\n",
    "\n",
    "# print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "# print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "# print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "# print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "# print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for train_index, val_index in kf.split(X):\n",
    "#     x_train, x_val = X[train_index], X[val_index]\n",
    "#     y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "#     clf = RandomForestClassifier(n_estimators=100)\n",
    "#     clf.fit(x_train, y_train)\n",
    "\n",
    "#     y_pred = clf.predict(x_val)\n",
    "#     y_pred_proba = clf.predict_proba(x_val)[:, 1]  # Get the probabilities for the positive class\n",
    "\n",
    "#     # Store true labels and predicted probabilities for ROC calculation\n",
    "#     y_true.extend(y_val)\n",
    "#     y_scores.extend(y_pred_proba)\n",
    "\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "#     precision = precision_score(y_val, y_pred)\n",
    "#     recall = recall_score(y_val, y_pred)\n",
    "#     f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "#     accuracy_results.append(accuracy)\n",
    "#     precision_results.append(precision)\n",
    "#     recall_results.append(recall)\n",
    "#     f1_results.append(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# average_accuracy = np.mean(accuracy_results)\n",
    "# average_precision = np.mean(precision_results)\n",
    "# average_recall = np.mean(recall_results)\n",
    "# average_f1 = np.mean(f1_results)\n",
    "\n",
    "# print(f'Average Accuracy: {average_accuracy:.4f}')\n",
    "# print(f'Average Precision: {average_precision:.4f}')\n",
    "# print(f'Average Recall: {average_recall:.4f}')\n",
    "# print(f'Average F1 Score: {average_f1:.4f}')\n",
    "\n",
    "# # Calculate ROC curve and AUC\n",
    "# fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # Plot ROC curve\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, color='blue', label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.0])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.savefig('roc_curve.png')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# X = np.hstack((train_m, train_s))\n",
    "# y = ys.flatten()\n",
    "\n",
    "# num_folds = 5\n",
    "# kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# accuracy_results = []\n",
    "# precision_results = []\n",
    "# recall_results = []\n",
    "# f1_results = []\n",
    "# y_true = []\n",
    "# y_scores = []\n",
    "\n",
    "\n",
    "# #balanced in randomforestclassifier\n",
    "\n",
    "\n",
    "# # Initialize lists to store evaluation metrics\n",
    "# accuracy_results, precision_results, recall_results, f1_results = [], [], [], []\n",
    "# y_true, y_scores = [], []\n",
    "\n",
    "# # KFold cross-validation\n",
    "# for train_index, val_index in kf.split(X):\n",
    "#     x_train, x_val = X[train_index], X[val_index]\n",
    "#     y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "#     # Add class weights to the classifier\n",
    "#     clf = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\")\n",
    "#     clf.fit(x_train, y_train)\n",
    "\n",
    "#     # Get predictions and probabilities\n",
    "#     y_pred = clf.predict(x_val)\n",
    "#     y_pred_proba = clf.predict_proba(x_val)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "#     # Store true labels and predicted probabilities for ROC calculation\n",
    "#     y_true.extend(y_val)\n",
    "#     y_scores.extend(y_pred_proba)\n",
    "\n",
    "#     # Calculate metrics at default threshold (0.5)\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "#     precision = precision_score(y_val, y_pred)\n",
    "#     recall = recall_score(y_val, y_pred)\n",
    "#     f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "#     # Append metrics to the results lists\n",
    "#     accuracy_results.append(accuracy)\n",
    "#     precision_results.append(precision)\n",
    "#     recall_results.append(recall)\n",
    "#     f1_results.append(f1)\n",
    "\n",
    "# # Calculate average metrics across all folds\n",
    "# avg_accuracy = sum(accuracy_results) / len(accuracy_results)\n",
    "# avg_precision = sum(precision_results) / len(precision_results)\n",
    "# avg_recall = sum(recall_results) / len(recall_results)\n",
    "# avg_f1 = sum(f1_results) / len(f1_results)\n",
    "# roc_auc = roc_auc_score(y_true, y_scores)  # Overall ROC-AUC score\n",
    "\n",
    "# print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "# print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "# print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "# print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "# print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# # Calculate ROC curve and AUC\n",
    "# fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # Plot ROC curve\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, color='blue', label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.0])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.savefig('roc_curve.png')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
